{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a09c0c-ccef-460a-8698-67d0603da1c2",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11130d2c-f27b-4d85-a250-63ef370ace02",
   "metadata": {},
   "source": [
    "The k-nearest neighbors (KNN) algorithm is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. It is one of the popular and simplest classification and regression classifiers used in machine learning today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e73e76-fd04-4861-b15f-8d7293449bd1",
   "metadata": {},
   "source": [
    "While the KNN algorithm can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf2b3d-1a11-4c98-92c5-859907f735d9",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a287f7f-c2bf-4ba2-a24a-a04c76cb3ea0",
   "metadata": {},
   "source": [
    "The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb6d8b6-6ef0-4f1e-8be8-8cf7e73bd800",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1de76-57e8-4313-b6c4-849bf7565084",
   "metadata": {},
   "source": [
    "Knn Classifier: Predicts a class by using the highest majority category among its k nearest neighbors. Knn Regression: Predicts a value by using the mean of the k nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ad764-1b18-46af-b686-2b7181d384bb",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e066fd8-60ae-4518-b946-c2847dd739e6",
   "metadata": {},
   "source": [
    "For classification : confusion matrix , Precision , Recall , Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8ce5f-03e8-435a-8fe5-d6fecbb85067",
   "metadata": {},
   "source": [
    "For regresssion : Mean Absoulate Error , Mean Square Error , Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62642cc-2ec4-4098-a520-1b16af0a8788",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567b341-2ce2-442b-93c3-6af5d77f3f71",
   "metadata": {},
   "source": [
    "When we add some features it will help to increase the accuracy but if we are adding more and more features and those features are useless ,it will decrease the accuracy , It is called curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9f596-1777-4132-a3a4-a86462ac48b6",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a96f1-2769-44ae-9709-6227adb09396",
   "metadata": {},
   "source": [
    "Preserves Relationships: KNN Imputer imputes missing values based on the nearest neighbors, which means it preserves the underlying relationships in the data. It takes into account the feature similarities between data points to estimate the missing values, making it more contextually relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271630c2-9785-4cb9-944f-dcab3fccec3a",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea702021-fe20-4c4c-a492-e8db63438a4c",
   "metadata": {},
   "source": [
    "KNN Regression : redicts the continuous value for an input based on the average (or weighted average) of the values of its k-nearest neighbors.\n",
    "It is used when The relationship between input features and the output is continuous and smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a705bf-0c2e-4752-9e2c-e94212293b59",
   "metadata": {},
   "source": [
    "KNN Classifier : enerally better suited for classification tasks where the output is a discrete label and the decision boundaries are complex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659abfcd-146a-4d60-8921-4b15e4af92ef",
   "metadata": {},
   "source": [
    "Choosing between KNN classifier and regressor depends on the nature of your problem. For discrete classification tasks, the KNN classifier is appropriate. For continuous prediction tasks, the KNN regressor is more suitable. Always consider the specific characteristics of your dataset and problem when deciding which version of KNN to use.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653673c-bacd-46f7-8488-047d15102aba",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8ad4-51ca-453f-ac07-445372d58146",
   "metadata": {},
   "source": [
    "Strength of KNN : \n",
    "Simplicity: KNN is easy to understand and implement. It requires no assumptions about the underlying data distribution.\n",
    "Flexibility: KNN can handle multi-class classification problems and can be applied to both binary and multi-class datasets.\n",
    "Adaptability: KNN can adapt to complex decision boundaries given sufficient data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b62e1-9521-4804-a54b-448a60181bda",
   "metadata": {},
   "source": [
    "Weekness of KNN : \n",
    "Memory Usage: KNN requires storing the entire training dataset, which can be memory-intensive.\n",
    "Curse of Dimensionality: The algorithm's performance can deteriorate in high-dimensional spaces because distances become less meaningful.\n",
    "Approximate Nearest Neighbors: Using algorithms that approximate nearest neighbors (e.g., Locality-Sensitive Hashing) can reduce computational time with a trade-off in precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64eb9e-bc11-4415-9ab8-a9b89195c45c",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490c236-da76-4e92-ae37-1f3001f91567",
   "metadata": {},
   "source": [
    "Euclidean distance is the shortest path between source and destination which is a straight line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e3cf0-f205-45cd-8cfe-c6b72c30c6e8",
   "metadata": {},
   "source": [
    " Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f520a0d7-acb2-4753-9857-9df1c9866877",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36169ebb-6a29-4c9a-b28a-427da176c526",
   "metadata": {},
   "source": [
    "To put it simply, yes, feature scaling is crucial for the KNN algorithm, as it helps in preventing features with larger magnitudes from dominating the distance calculations. Feature scaling is an essential step in the data preprocessing pipeline, especially for distance-based algorithms like the KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f71eb-6ad6-42e2-92ed-83cac0d10e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
